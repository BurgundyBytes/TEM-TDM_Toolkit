# TEM-TDM Toolkit
This Python toolkit is designed to assess the validity and performance of Asynchronous Sigma-Delta Modulators (ASDM) when used as Time Encoding Machines (TEM) and Time Decoding Machines (TDM), particularly for complex or turbulent signals. 

It implements and analyzes the concepts presented in the paper **[Perfect Recovery and Sensitivity Analysis of Time Encoded Bandlimited signals](docs/resources/Perfect-Recovery-and-Sensitivity-Analysis-of-Time-Encoded-Bandlimited-signals.pdf)** by Lazar and Tóth (IEEE TCAS-I, 2004), developed by researchers in the [Bionet Group at Columbia University](http://www.bionet.ee.columbia.edu/research/nipm/tems). The toolkit leverages the paper's normalization technique to manage the complexity of the ASDM's three physical parameters (`k`, `b`, `delta`).

![Normalized TEM Equivalent Circuit Diagram](docs/resources/ASDM_Equivalent_Circuit.PNG)

Reducing the degrees of freedom in a physical system analysis, like going from (`k`, `b`, `delta`) to fewer parameters, doesn't eliminate the underlying physics; instead, it requires introducing constraints. For feedback systems like the ASDM, especially nonlinear ones, these constraints often relate to **stability** and **error performance**. This toolkit explores how these constraints manifest when using the normalized model. Normalization forces us to pay close attention to how the signal enters the circuit (via the **bias `b`**, ensuring `b > max|x(t)|` for stability) and how the chosen parameters interact to ensure reliable signal recovery (via the **perfect recovery criterion `r < 1`**). The analysis therefore revolves around finding the optimal balance between the normalized threshold **`d_norm`** and the bias **`b`**.

> [!NOTE]
> **Developer's Note:** 

## Core Concepts 

1.  **Time Encoding/Decoding (TEM/TDM):** Representing signal amplitude via asynchronous spike timings.
2.  **Asynchronous Sigma-Delta Modulator (ASDM):** The physical circuit (gain `k`, bias `b`, threshold `delta`) used as the TEM.
3.  **Normalization & Constraints:** Using the paper's normalized model simplifies analysis but introduces implicit constraints:
-  **Stability:** The physical bias `b` *must* be greater than the peak signal amplitude `c`.
- **Recovery:** The perfect recovery condition `r = (d_norm / (1 - c/b)) * (Ω / π) < 1` links the normalized threshold `d_norm`, the *bias ratio* `b/c`, and the signal bandwidth `Ω`. Satisfying this is essential for accurate decoding.
4.  **Key Parameters for Sizing (`b`, `d_norm`):** Due to the constraints, finding the optimal *physical* setup involves effectively tuning `b` (while ensuring stability `b>c`) and `d_norm` (while ensuring recovery `r<1`). These become the two primary degrees of freedom explored in the parametric studies.
5.  **Parametric "Sizing":** This toolkit performs parametric studies varying `b` and `d_norm` (individually or combined) to map the performance landscape. The goal is to "size" the modulator by finding parameter combinations that optimize desired error metrics (e.g., Median Error, NRMSE) while respecting computational limits (time threshold) and performance bounds (error threshold).
6.  **Optimal Selection:** Based on the parametric results, an "optimal" combination of (`b`, `d_norm`) is selected according to user-defined criteria (e.g., minimize median error within time/error limits).
7.  **Comparison Analyses:**
- **Nyquist:** Benchmarks the optimally configured ASDM against traditional, amplitude-based, equispaced sampling (reconstructed via splines) over a range of sampling points (`N`) from the Nyquist limit up to the number of spikes generated by the optimal ASDM (`N_spikes_opt`).
- **Fourier:** Compares the frequency spectra of the original signal, the optimal ASDM reconstruction, and the traditional reconstruction (at `N = N_spikes_opt`) to assess information preservation and potential distortion in the frequency domain, based on the principle that signals can be represented by orthogonal frequency components.

## Project Structure

```plaintext
├── docs                     # Documentation and diagrams
│   ├── managers             # Managers' documentation
│   ├── functionalities      # Core functions' documentation
│   ├── resources            # Resources accessible to the toolkit: images, .pfd, ...
│   └── project.md           # Toolkit overview
├── src                      # Source code
│   ├── controllers          # Handlers for configuration, input, studies, results
│   │   ├── configuration.py
│   │   ├── input_handler.py
│   │   ├── parametric_handler.py
│   │   └── results_handler.py
│   ├── models               # Signal generation/loading functions
│   │   └── input_signal.py
│   ├── parametric           # Parametric study implementation
│   │   └── studies.py
│   ├── analysis             # Nyquist, Fourier, Optimal conditions analysis
│   │   ├── nyquist.py
│   │   ├── fourier.py
│   │   └── optima.py
│   ├── utilities            # Core ASDM, metrics, logging, plotting, helpers
│   │   ├── asdm.py
│   │   ├── metrics.py
│   │   ├── logging.py
│   │   ├── plotting.py
│   │   └── utils.py
├── Input                    # Folder for input data files (e.g., signal.csv)
├── Output                   # Folder for all generated results (plots, data, logs)
│   ├── parametric_delta     # Results for delta parametric study
│   ├── parametric_bias      # Results for delta parametric study
│   ├── parametric_biasDelta # Results for biparametric study
│   ├── nyquist              # Results for Nyquist analysis
│   ├── fourier              # Results for Fourier analysis
│   ├── optima               # Results for optimal conditions study
├── launcher.py              # Main script to run the toolkit
├── config.txt               # Configuration file for specifying runs
├── requirements.txt         # Python package dependencies
└── TEM-TDM_Toolkit.ipynb    # Code to be run in Google Colab
```

## Repository explanation: Manufacturing plant analogy

I like to think of TEM-TDM as a signal-processing manufacturing plant that receives a project order with detailed requirements (`config.txt`). Based on this order, the plant manager (`launcher.py`) determines which **workflow** to activate: *'execute'* or *'results'*.
* The *'execute'* workflow is the plant's production line. It generates inventory (the results of parametric studies) as specified in the project order (`config.txt`). The raw materials for this production are input signals, which can come from external suppliers (experimental data in `.csv` or `.xlsx` files) or be manufactured in-house (generated signals like `multisin`, `multifreq`, and `paper`).
* The *'results'* workflow is the plant's analysis department. It consumes inventory (the results of parametric studies). This inventory can be freshly produced by the *'execute'* workflow or retrieved from the plant's warehouse (the *Output folders*, from past studies).

Both workflows have their own **department managers**. 
* The *'execute'* workflow, for instance, calls upon the *parametric studies manager*, who organizes the production according to the order: running the studies, logging data, and creating plots.
* The *'results'* workflow calls the *results manager*, who organizes the specified analyses: the Nyquist study, the Fourier Analysis, the optimal conditions study, and the generation of plots.

```mermaid
%%{ init: { 'theme': 'base', 'themeVariables': { 'primaryColor': '#ffffff', 'primaryTextColor': '#333', 'lineColor': '#666', 'fontSize': '14px' } } }%%
graph LR
    %% -------------------------------- Styles (Using established template) --------------------------------
    classDef inputStyle fill:#e3f2fd,stroke:#90caf9,stroke-width:1px,color:#333,rx:4,ry:4
    classDef processStyle fill:#fff,stroke:#888,stroke-width:1.5px,color:#000,font-weight:bold,rx:4,ry:4
    classDef outputStyle fill:#e8f5e9,stroke:#a5d6a7,stroke-width:1px,color:#333,rx:4,ry:4
    %% Optional: A slightly distinct style for the 'Manager' node
    classDef managerStyle fill:#fffde7,stroke:#fdd835,stroke-width:1.5px,color:#000,font-weight:bold,rx:4,ry:4

    %% -------------------------------- Diagram: Manufacturing Plant Analogy --------------------------------

    subgraph "Main Workflow"
        direction LR
        Order[("fa:fa-file-alt Project Order<br>(config.txt)")]:::inputStyle
        PlantManager{{"fa:fa-user-tie Plant Manager<br>(launcher.py)"}}:::managerStyle
        ParametricStudy[("fa:fa-cogs Parametric Study<br>(Execute Workflow)")]:::processStyle
        ResultsAnalysis[("fa:fa-chart-line Results Analysis<br>(Results Workflow)")]:::processStyle
    end

    subgraph "Results Storage / Inventory"
        direction TB
        StudyResults[("fa:fa-database Study Results<br>(Parametric Data, Metrics)")]:::outputStyle
        AnalysisResults[("fa:fa-file-image Analysis Outputs<br>(Plots, Reports)")]:::outputStyle 
    end

    %% -------------------------------- Connections --------------------------------
    Order -- "Provides Specs" --> PlantManager
    PlantManager -- "Activates 'Execute'" --> ParametricStudy
    PlantManager -- "Activates 'Results'" --> ResultsAnalysis

    ParametricStudy -- "Generates/Stores" --> StudyResults
    ResultsAnalysis -- "Generates/Stores" --> AnalysisResults
    StudyResults -- "Consumed by" --> ResultsAnalysis
```

## How it Works: The Signal Processing Plant Analogy

Thinking of the TEM-TDM Toolkit as a specialized manufacturing plant for processing and analyzing signals using ASDM techniques, one can further understand how the tool works. As a manufacturing plant, the code follows defined steps:

1.  **Receiving the Project Order (`config.txt`):** The entire operation starts with a detailed project order or blueprint (`config.txt`). This document specifies exactly what needs to be done: the raw material (which input signal to use), the production processes required (which parametric studies to run), the quality control checks needed (which analyses like Nyquist, Fourier, Optima to perform), and the final deliverables (plots, data files, logs).

2.  **Planning Department (`configuration.py`):** The `configuration.py` module acts as the planning department. It reads and interprets the blueprint (`config.txt`), ensuring all instructions are clear and setting up the necessary parameters and output locations for the plant's operations.

3.  **Activating Workflows (`launcher.py`):** The plant manager (`launcher.py`) reviews the interpreted blueprint and activates one or both of the main workflows as specified:
    *   **Production Line (`Execution Flow`):** If activated, this workflow focuses on *generating* data.
        *   **Receiving/Internal Manufacturing (`input_handler.py`):** This department secures the raw signal, either by loading it from external suppliers (Excel/CSV files in `Input/`) or by manufacturing it internally using standard specifications (`models/input_signal.py`).
        *   **Production Floor Supervision (`parametric_handler.py`):** This supervisor oversees the running of parametric studies. It instructs the simulation workshop (`parametric/studies.py`) to perform the core encoding and decoding simulations for the two degrees of freedon: `b` and `d_norm`. It uses the quality control lab (`utilities/metrics.py`) to measure performance during production. The results of these simulations (metrics vs. parameters) are considered intermediate goods.
    *   **Analysis & Reporting (`Results Flow`):** If activated, this workflow focuses on *analyzing* data to produce final insights.
        *   **Analysis Department Management (`results_handler.py`):** This manager coordinates the post-processing analysis. It retrieves the necessary intermediate goods (parametric study results), *either* fresh from the production line (if `Execution Flow` just ran) *or* from previous runs stored in the warehouse (`Output/`). It then dispatches tasks to specialized analysis teams:
            *   Optimization Team (`analysis/optima.py`): Finds the best operating parameters (`b`, `d_norm`) based on criteria from the blueprint (if any thresholds were given in the elapsed time on the encoding/decoding process or the median error).
            *   Nyquist Benchmarking Team (`analysis/nyquist.py`): Compares ASDM performance against traditional uniform sampling.
            *   Fourier Benchmarking Team (`analysis/fourier.py`): Analyzes the signal's fidelity in the frequency domain.

4.  **Specialized Workshops & Services (Core Modules):** Various modules perform the hands-on work. I like to think that these are the modules whose functions can be thought of as single tasks in a manufacturing plant. They perform the minimum units of execution and involve pure technical work, with not a glimpse of managing in their definition.
    *   **Core ASDM Machine (`utilities/asdm.py`):** The fundamental TEM/TDM encoder/decoder.
    *   **Graphics Department (`utilities/plotting.py`):** Creates all plots and visualizations.
    *   **Record Keeping (`utilities/logging.py`):** Documents the entire process.
    *   **General Tools (`utilities/utils.py`):** Provides essential support functions (file I/O, FFT, etc.).

5.  **Warehouse (`Output/`):** The `Output/` folder serves as the plant's warehouse. It stores *all* generated outputs in an organized manner:
    *   Intermediate Goods: Parametric study results (metrics vs. parameters) stored in dedicated subfolders (e.g., `parametric_delta/`), often as efficient `.pkl` files and/or `.xlsx` summaries.
    *   Final Products: Analysis reports, plots comparing signals or spectra, optimal condition summaries, stored in their respective analysis subfolders (e.g., `nyquist/`, `fourier/`, `optima/`).
    *   Process Logs: Detailed text logs documenting each run.

6.  **Shipping:** The plant delivers the final requested outputs (plots, data summaries, optimal parameters) as specified in the original blueprint (`config.txt`) and stored in the `Output/` warehouse.


## Installation

You can run this toolkit either locally on your machine or directly within Google Colab. **Running on Google Colab is recommended** for easier setup and access to computational resources.

### Method 1: Google Colab (Recommended)

1.  **Open Google Colab:** Go to [colab.research.google.com](https://colab.research.google.com).
2.  **Upload the Project:** Upload the file `TEM-TDM_Toolkit.ipynb` directly con Google Colab (`File -> Upload notebook`). The rest of the project files (`src`, `Input`, `config.txt`, etc.) need to be accessible. Place them in your Google Drive.
3.  **Mount Google Drive:** Add or run the following cell on the `.ipynb` file, to access your Drive files:
    ```python
    from google.colab import drive
    drive.mount('/content/drive')
    ```
    Follow the authentication prompt.
4.  **Navigate to Project Directory:** Use the `%cd` magic command in a code cell to change to the directory where your project files are located within your mounted Drive. Example:
    ```python
    %cd /content/drive/MyDrive/Colab Notebooks/TEM-TDM Toolkit
    ```
    *(Adjust the path according to where you saved the project in your Drive).*
5.  **Install Dependencies:** Run the following in a code cell:
    ```python
    !pip install -r requirements.txt
    ```
6.  **Configure:** You can edit the `config.txt` file directly through the Colab file browser (left sidebar) or use code to modify it if needed.
7.  **Run:** Execute the main launcher script in a code cell:
    ```python
    !python launcher.py
    ```
8.  **Access Output:** Results will be saved to the `Output/` folder within your project directory on Google Drive.

### Method 2: Local Installation
> Ensure you have Python 3.8 or newer installed.
1.  **Clone the repository:**
    ```bash
    git clone https://github.com/BurgundyBytes/TEM-TDM_Toolkit.git
    cd TEM-TDM_Toolkit
    ```
2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv\Scripts\activate  # On Linux use `venv/bin/activate`
    ```
3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Usage

1.  **Prepare Input:** Place experimental data files (if any) in the `Input/` folder.
2.  **Configure:** Edit `config.txt` to define the desired workflow, input signal, parametric study parameters, and analyses.
3.  **Run:** Execute the launcher script from the terminal:
    ```bash
    python launcher.py
    ```
4.  **Check Output:** Results (data files, plots, logs) will be generated in the `Output/` directory, organized into subfolders based on the study or analysis type.

### The Project Order (`config.txt`)

The toolkit's behavior is entirely controlled by the `config.txt` file. 
This file uses a simple `Key: Value` format, with comments denoted by `#`. Before running `launcher.py`, you should edit this file to specify:

1.  **Workflow:** Whether to run parametric simulations (`Execution Flow`), perform analysis on results (`Results Flow`), or both.
2.  **Input Signal:** Where to get the signal (`Input Source`: from a file or generated) and its specific parameters (filename, sampling rate, generation type, duration, frequencies, etc.).
3.  **Output Control:** Where to save results (`Output Folder` and specific subfolders) and whether to generate plots, text logs, and pickle files.
4.  **Parametric Studies:** Which parameter sweeps to perform (`run_parametric_delta`, `run_parametric_bias`, `run_biparametric`). Define the ranges (`delta_range`, `bias_range`). Crucially, the **`bias_range` must only contain values `b > c`** (where `c = max(|x(t)|)`). Define default values for non-swept parameters.
5.  **Analysis Tasks:** Which analyses to run after the simulations (`Run Optimal`, `Run Nyquist`, `Run Fourier`) and any thresholds used for finding optimal conditions.

**Refer to the detailed comments within the `config.txt` file itself for explanations of each specific parameter.**


## A Note on Project Evolution and Scope

This toolkit represents a significant part of my final degree project (TFG), but it wasn't the initial plan. My degree project involves both understanding the theory behind ASDM encoding/decoding for turbulent signals (based on the Bionet group's research) and conducting experiments at INTA to characterize turbulence using a sweeping jet actuator.

Early on, I realized that simply pursuing the theoretical and experimental parts separately wouldn't be as impactful or satisfying. I wanted to actively connect them and create something tangible and meaningful. This led to the idea of building this toolkit – refactoring my initial simulation codes into a flexible and scalable tool.

**This refactoring effort, and the design philosophy behind it, wouldn't have been possible without the experiences gained at my workplace and the invaluable guidance of Fernando.** The constant need to automate processes requires to identify patterns, group functions into logical tasks and even workflows. He fostered a deeper appreciation for Python as a versatile tool – a true canvas for building complex solutions. It is a true skill to abstract technical details to build flexible systems, which he is a master of. Surrounded by this rich environment, the tool could only be born. This experience directly inspired the concept of structuring this toolkit like a "manufacturing plant", with distinct workflows, managers, and specialized units.

As the workload from my cousework progressed and complexity of the analysis increased, I needed an efficient way to run various simulations, manage data interactions, and store results without getting bogged down. This toolkit became the bridge, allowing me to test theoretical concepts (like the ASDM parameters) and potentially apply them to experimental data contexts later.

This exploration led to the current focus on the **bias (`b`)** and **normalized threshold (`d_norm`)** as the key parameters for "sizing" the ASDM encoder, constrained by stability (`b>c`) and perfect recovery (`r<1`). The toolkit is therefore built to perform parametric studies on these two parameters, individually or together, allowing a search for the optimal operating point that minimizes a chosen error metric within computational constraints.

Building this toolkit has been an invaluable learning experience, significantly enhancing my understanding of the underlying theory. Although it emerged from the specific needs of my degree project, I believe its structured approach could be beneficial to others in the research department or anyone undertaking similar signal-processing explorations. **I hope this repository serves as a useful example and a building block for bigger and better things.**


## Future Directions and Potential Research Paths

While this toolkit provides a robust framework for analyzing ASDM-based time encoding, I can sense some exciting paths for future research and development:

1.  **Refining Physical Parameter Selection (`k`, `b`, `delta`):**
    
    The current approach translates the optimal normalized parameters (`b`, `d_norm`) back to physical ones (`k`, `b`, `delta`) by applying stability (`b>c`) and recovery (`r<1`) constraints, often requiring iterative adjustments.

    Could more formal methods, perhaps inspired by control theory techniques for tuning controllers like PIDs (e.g., Ziegler-Nichols methods, frequency response analysis), be adapted? 
    
    These methods analytically or empirically determine controller gains based on system response. Exploring if the ASDM's feedback dynamics share characteristics that allow similar tuning approaches for `k`, `b`, and `delta` could maybe lead to more direct and optimized physical parameter selection.

2.  **Physics-Informed Parameter Prediction for Specific Signals:**
    
    For specific signal classes like turbulence, the underlying physics dictates relationships between key quantities. Dimensional analysis, particularly using the **Buckingham Pi theorem**, allows identifying fundamental dimensionless groups (e.g., Reynolds number, Strouhal number, pressure coefficients for aerodynamic signals).
    
    Could these dimensionless groups, which characterize the signal's behavior, be correlated with the optimal `d_norm` or the optimal bias ratio `b/c` required for efficient encoding? 
    
    Finding such a relationship could allow predicting near-optimal encoding parameters directly from the physical characteristics of the flow or signal source, significantly reducing the need for extensive parametric sweeps. 
    
    This represents the beautiful interplay between physics and mathematics.

3.  **Hardware Implementation and Validation:**
    
    Building a physical circuit based on the selected `k`, `b`, `delta` parameters and validating its performance against simulations using real-world signals (including the experimental sweeping jet data) would be the ultimate test of the methodology.

4.  **Exploring Alternative TEM Architectures:**
    
    Applying the toolkit's analysis framework (parametric studies, comparisons) to other TEM models proposed by the Bionet group or elsewhere (e.g., Integrate-and-Fire neurons with different adaptation mechanisms) could provide valuable comparative insights.
